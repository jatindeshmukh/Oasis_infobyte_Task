# -*- coding: utf-8 -*-
"""OASIS INFOBYTE TASK5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SgAWnvaEeTVhhINtJQTXvGQf7ogOjEeM
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import nltk
import os
sns.set_style('whitegrid')
import warnings
warnings.filterwarnings('ignore')

#load dataset from csv file and use encoding as latin-1
email_df=pd.read_csv("/content/spam (2).csv",encoding='latin-1')

#print first 5 records
email_df.head()

#print concise summary about dataset.
email_df.info()

#lets delete Unnamed : 2 , Unnamed : 3 and Unnamed : 4 column because they are having zero values in almost entire column.
column_to_delete=[name for name in email_df.columns if name.startswith('Unnamed')]
email_df.drop(columns=column_to_delete,inplace=True)

#rename v1 column to target and v2 column to message
email_df.rename(columns=dict({"v1":"target","v2":"message"}),inplace=True)

#after deleting and renaming columns print last 5 records of the dataset
email_df.tail()

#print null values
email_df.isnull().sum()

#print no of duplicate records
print("Total duplicated records in dataset are : {}".format(email_df.duplicated().sum()))

#lets remove duplicated records
email_df.drop_duplicates(inplace=True)

"""Data Preprocessing :"""

#function to map target with 0 and 1
def target_mapper(text):
    return 0 if text=='spam' else 1

email_df["target"]=email_df['target'].apply(func=target_mapper)

#import nltk library for data preprocessing
import nltk
nltk.download('punkt') #download punctuation
nltk.download('stopwords') #download stopwords
from nltk.corpus import stopwords
from nltk import PorterStemmer
from nltk import tokenize  #import nltk tokenize package to use word and sentence tokenizer.
STOPWORDS=stopwords.words("english") #taking only english stopwords because spam messages are in english language only.

#function to preprocess row text into ready to use format for our model.
def message_tranformation(text):
    text=text.strip() #remove black spaces from starting and ending of message text
    text=text.lower() #coverting all message words into lowercase format to generalize it.

    #tokenize the text
    words=tokenize.word_tokenize(text)

    #intialize the porter stemmer
    stemmer=PorterStemmer()

    #remove stopwords and applying stemming while ignoring special words.
    filtered_words=[stemmer.stem(word) for word in words if word not in STOPWORDS and word.isalnum()]

    #Join list of filter words back to the string format
    transformed_text=" ".join(filtered_words)
    return transformed_text

#applying message tranformation function on email_df transformed_message column.
email_df["transformed_message"]=email_df["message"].apply(message_tranformation)

#print first 5 recods after cleaning messages
email_df.head()

#now we can delete message column because we are going to work with preprocessed text messages only.
email_df.drop(columns="message",inplace=True)

#configure object of wordcloud plot
from wordcloud import WordCloud
wc=WordCloud(width=1000,height=1000,min_font_size=8,background_color='white')

"""Wordcloud for Spam Category messaages"""

#generate wordcloud plot for spam messages
spam_wc=wc.generate(email_df[email_df["target"]==0]["transformed_message"].str.cat(sep=" "))
plt.figure(figsize=(20,5))
plt.imshow(spam_wc)
plt.show()

"""Wordcloud of ham category"""

#generate wordcloud plot for not-spam messages
ham_wc=wc.generate(email_df[email_df["target"]==1]["transformed_message"].str.cat(sep=" "))
plt.figure(figsize=(20,5))
plt.imshow(ham_wc)
plt.show()

#used words in spam messages
spam_corpus=list()
for msg in email_df[email_df['target']==0]["transformed_message"].to_list():
    for word in msg.split():
        spam_corpus.append(word)

len(spam_corpus)

#print the most common 50 words from the spam category messages
from collections import Counter
spam_top_50_common_words=pd.DataFrame(Counter(spam_corpus).most_common(50))
print(spam_top_50_common_words)

#used words in ham messages
ham_corpus=list()
for msg in email_df[email_df['target']==1]["transformed_message"].to_list():
    for word in msg.split():
        ham_corpus.append(word)

len(ham_corpus)

#most commnaly used 50 words from ham category messages
ham_top_50_common_words=pd.DataFrame(Counter(ham_corpus).most_common(50))
print(ham_top_50_common_words)

"""Data Transformation"""

from sklearn.feature_extraction.text import CountVectorizer
cVector=CountVectorizer() #CountVectorizer is used to convert text into numeric array
x=cVector.fit_transform(email_df["transformed_message"]).toarray()

#seperating target column
y=email_df['target']

#check the distribution of target variable using Pie chart
plt.pie(y.value_counts().values,labels=["Not Spam","Spam"],autopct="%0.2f%%")
plt.show()